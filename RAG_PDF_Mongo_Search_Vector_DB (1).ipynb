{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://www.mongodb.com/pt-br/docs/atlas/atlas-vector-search/rag/"
      ],
      "metadata": {
        "id": "_0tBwJ3eKR0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing the packages"
      ],
      "metadata": {
        "id": "C99L9atrKcha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4w48d7iKPFi",
        "outputId": "f4108793-2c44-42c9-e8da-d896891a7780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m1.3/1.7 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/275.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.7/298.7 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.4/412.4 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet --upgrade pymongo sentence_transformers einops langchain langchain_community pypdf huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/phytotherapy-20250127T195124Z-001.zip"
      ],
      "metadata": {
        "id": "JfybvZZBWsMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gives ip address\n",
        "!hostname -I\n",
        "\n",
        "#gives ip address\n",
        "!curl ipecho.net/plain\n",
        "\n",
        "#Gives ip addresses with port numbers\n",
        "!sudo lsof -i -P -n | grep LISTEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzgxV1CKUVjr",
        "outputId": "526f1653-4c10-4b26-9089-5bc1d5dd7de4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172.28.0.12 \n",
            "34.53.1.238node         6 root   21u  IPv6  22623      0t0  TCP *:8080 (LISTEN)\n",
            "kernel_ma   18 root    3u  IPv4  21215      0t0  TCP 172.28.0.12:6000 (LISTEN)\n",
            "colab-fil   65 root    3u  IPv4  21323      0t0  TCP 127.0.0.1:3453 (LISTEN)\n",
            "jupyter-n  114 root    7u  IPv4  23747      0t0  TCP 172.28.0.12:9000 (LISTEN)\n",
            "python3   1096 root   21u  IPv4  51899      0t0  TCP 127.0.0.1:44561 (LISTEN)\n",
            "python3   1131 root    3u  IPv4  52264      0t0  TCP 127.0.0.1:36635 (LISTEN)\n",
            "python3   1131 root    5u  IPv4  52189      0t0  TCP 127.0.0.1:53965 (LISTEN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Injesting the data on the Atlas"
      ],
      "metadata": {
        "id": "RvM1lYo9Kiol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the embedding model (https://huggingface.co/nomic-ai/nomic-embed-text-v1\")\n",
        "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\n",
        "\n",
        "# Define a function to generate embeddings\n",
        "def get_embedding(data):\n",
        "    \"\"\"Generates vector embeddings for the given data.\"\"\"\n",
        "\n",
        "    embedding = model.encode(data)\n",
        "    return embedding.tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwUsJ9uPKSXS",
        "outputId": "93a1d7ca-17eb-4096-b205-f8852a034991"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.nomic-ai.nomic-bert-2048.40b98394640e630d5276807046089b233113aa87.modeling_hf_nomic_bert:<All keys matched successfully>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load the PDF\n",
        "loader = PyPDFLoader(\"https://investors.mongodb.com/node/12236/pdf\")\n",
        "data = loader.load()\n",
        "\n",
        "# # Split the data into chunks\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=20)\n",
        "# documents = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "-J0GOqoRKnoL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# Connect to your Atlas cluster\n",
        "client = MongoClient(\"mongodb+srv://USER:PASSWORD@cluster0.acuucyq.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\")\n",
        "collection = client[\"microgreens\"][\"papers\"]\n",
        "\n",
        "def insert_documents(documents):\n",
        "    # Prepare documents for insertion\n",
        "    docs_to_insert = [{\n",
        "        \"text\": doc.page_content,\n",
        "        \"embedding\": get_embedding(doc.page_content)\n",
        "    } for doc in documents]\n",
        "\n",
        "    result = collection.insert_many(docs_to_insert)\n"
      ],
      "metadata": {
        "id": "NJwmm9voM11n"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: a method to read the google drive path and read all the pdfs available on the shared path: https://drive.google.com/drive/folders/12NRMYQ4sXvvxZNLE9UYjTFh-BU-24GBo?usp=sharing\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "def read_pdfs_from_drive(folder_path):\n",
        "    \"\"\"Reads all PDF files from a specified Google Drive folder.\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            filepath = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                loader = PyPDFLoader(filepath)\n",
        "                # documents.extend(loader.load())\n",
        "                documents = text_splitter.split_documents(loader.load())\n",
        "                insert_documents(documents)\n",
        "\n",
        "                print(f\"Loaded {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {filename}: {e}\")  # Handle potential errors gracefully\n",
        "    return documents\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "\n",
        "# Now you can process the 'all_pdf_documents' as needed.\n",
        "# For example, you can split them into chunks:\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=20)\n",
        "# all_documents = text_splitter.split_documents(all_pdf_documents)"
      ],
      "metadata": {
        "id": "1O9gf2YbO6H-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shared_folder_path = f\"/content/drive/MyDrive/microgreens/\"\n",
        "all_pdf_documents = read_pdfs_from_drive(shared_folder_path)"
      ],
      "metadata": {
        "id": "Nth3UDBVY1m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fQCsea5jPi2w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60a35b73-c89a-4501-b006-b9f692e31be1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo.operations import SearchIndexModel\n",
        "import time\n",
        "\n",
        "# Create your index model, then create the search index\n",
        "index_name=\"vector_index\"\n",
        "search_index_model = SearchIndexModel(\n",
        "  definition = {\n",
        "    \"fields\": [\n",
        "      {\n",
        "        \"type\": \"vector\",\n",
        "        \"numDimensions\": 768,\n",
        "        \"path\": \"embedding\",\n",
        "        \"similarity\": \"cosine\"\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  name = index_name,\n",
        "  type = \"vectorSearch\"\n",
        ")\n",
        "collection.create_search_index(model=search_index_model)\n",
        "\n",
        "# Wait for initial sync to complete\n",
        "print(\"Polling to check if the index is ready. This may take up to a minute.\")\n",
        "predicate=None\n",
        "if predicate is None:\n",
        "   predicate = lambda index: index.get(\"queryable\") is True\n",
        "\n",
        "while True:\n",
        "   indices = list(collection.list_search_indexes(index_name))\n",
        "   if len(indices) and predicate(indices[0]):\n",
        "      break\n",
        "   time.sleep(5)\n",
        "print(index_name + \" is ready for querying.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTCr_veG5pFs",
        "outputId": "4dd3073a-d781-4dbe-d743-ff00b17bc50f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polling to check if the index is ready. This may take up to a minute.\n",
            "vector_index is ready for querying.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "# Load the embedding model (https://huggingface.co/nomic-ai/nomic-embed-text-v1\")\n",
        "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\n",
        "\n",
        "# Define a function to generate embeddings\n",
        "def get_embedding(data):\n",
        "    \"\"\"Generates vector embeddings for the given data.\"\"\"\n",
        "    embedding = model.encode(data)\n",
        "    return embedding.tolist()\n",
        "\n",
        "# Define a function to run vector search queries\n",
        "def get_query_results(query):\n",
        "  \"\"\"Gets results from a vector search query.\"\"\"\n",
        "\n",
        "  query_embedding = get_embedding(query)\n",
        "  pipeline = [\n",
        "      {\n",
        "            \"$vectorSearch\": {\n",
        "              \"index\": \"vector_index\",\n",
        "              \"queryVector\": query_embedding,\n",
        "              \"path\": \"embedding\",\n",
        "              \"exact\": True,\n",
        "              \"limit\": 5\n",
        "            }\n",
        "      }, {\n",
        "            \"$project\": {\n",
        "              \"_id\": 0,\n",
        "              \"text\": 1\n",
        "         }\n",
        "      }\n",
        "  ]\n",
        "\n",
        "  results = collection.aggregate(pipeline)\n",
        "\n",
        "  array_of_results = []\n",
        "  for doc in results:\n",
        "      array_of_results.append(doc)\n",
        "  return array_of_results"
      ],
      "metadata": {
        "id": "yPRFz0TYOJLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "932b0128-727e-41dc-d7e7-7b2b441e0b6d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.nomic-ai.nomic-bert-2048.40b98394640e630d5276807046089b233113aa87.modeling_hf_nomic_bert:<All keys matched successfully>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function with a sample query\n",
        "import pprint\n",
        "pprint.pprint(get_query_results(\"methodology cultivate\"))"
      ],
      "metadata": {
        "id": "LtO99hPJ4VAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install groq\n",
        "import os\n",
        "from groq import Groq\n",
        "from pprint import pprint\n",
        "os.environ[\"GROQ_API_KEY\"] = \"\"\n",
        "\n",
        "# Specify search query, retrieve relevant documents, and convert to string\n",
        "query = \"give a recipe using the microgreens brocoli and talk about the healthy benefits\"\n",
        "context_docs = get_query_results(query)\n",
        "context_string = \" \".join([doc[\"text\"] for doc in context_docs])\n",
        "\n",
        "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
        "\n",
        "prompt = f\"\"\"Use the following pieces of context to answer the question at the end.\n",
        "    {context_string}\n",
        "    Question: {query}\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the GroqClient with your API token\n",
        "\n",
        "# Send the prompt to the LLaMA 3.3 model\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "    temperature=1,\n",
        "    max_completion_tokens=1024,\n",
        "    top_p=1,\n",
        "    stream=True,\n",
        "    stop=None,\n",
        ")\n",
        "\n",
        "for chunk in completion:\n",
        "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LlINCGsy6EZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G79kZzf8AM8g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}